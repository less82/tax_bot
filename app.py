import streamlit as st
import os
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.chains import create_history_aware_retriever

load_dotenv()

st.set_page_config(page_title="2025 ì—°ë§ì •ì‚° ìƒë‹´ ì±—ë´‡", page_icon="ğŸ’°")

st.title("ğŸ’° 2025ë…„ ì—°ë§ì •ì‚° ìƒë‹´ ì±—ë´‡")
st.markdown("""
2024ë…„ ê·€ì† ì—°ë§ì •ì‚° ì‹ ê³ ì•ˆë‚´ ë° 2025ë…„ ê°œì •ì„¸ë²•ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•´ë“œë¦½ë‹ˆë‹¤.\n
**ì£¼ì˜:** ì •í™•í•œ ì„¸ë¬´ ìƒë‹´ì€ ì „ë¬¸ê°€ì™€ ìƒì˜í•˜ì„¸ìš”.
""")

# Initialize Chat Chain
@st.cache_resource
def get_chain():
    # 1. Setup Vector Store
    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma(persist_directory="chroma_db", embedding_function=embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    
    # 2. Setup LLM
    llm = ChatOpenAI(temperature=0, model_name="gpt-4o")

    # 3. Contextualize Question (History Aware Retriever)
    # This chain rewrites the question based on history to make it standalone
    contextualize_q_system_prompt = """Given a chat history and the latest user question \
    which might reference context in the chat history, formulate a standalone question \
    which can be understood without the chat history. Do NOT answer the question, \
    just reformulate it if needed and otherwise return it as is."""
    
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )
    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )

    # 4. Answer Question (Stuff Documents Chain)
    # This chain takes the documents and the question and generates the answer
    qa_system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ ì„¸ë¬´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë¬¸ë§¥(context)ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
    
    ë¬¸ë§¥:
    {context}
    
    ê·œì¹™:
    1. 2025ë…„ ê°œì •ì„¸ë²• ë‚´ìš©ì´ ìˆë‹¤ë©´ ì´ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ë°˜ì˜í•˜ì„¸ìš”.
    2. ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©ì€ ì§€ì–´ë‚´ì§€ ë§ê³  "ì œê³µëœ ìë£Œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.
    3. ì¹œì ˆí•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
    4. ê´€ë ¨ëœ ë‚´ìš©ì´ 2024ë…„ ìë£Œì™€ 2025ë…„ ê°œì •ì•ˆì— ëª¨ë‘ ìˆë‹¤ë©´, ê°œì •ì•ˆì„ ê¸°ì¤€ìœ¼ë¡œ ì„¤ëª…í•˜ê³  ë³€ê²½ ì „ ë‚´ìš©ë„ ê°„ëµíˆ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.
    
    ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”."""
    
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

    # 5. Final Retrieval Chain
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    
    return rag_chain

# Session State for Chat History
if "messages" not in st.session_state:
    st.session_state.messages = []

if "store" not in st.session_state:
    st.session_state.store = {}

session_id = "user_session"
if session_id not in st.session_state.store:
    st.session_state.store[session_id] = ChatMessageHistory()

# Helper to manage history manually since Streamlit reruns
def get_session_history(session_id: str) -> BaseChatMessageHistory:
    return st.session_state.store[session_id]

chain = get_chain()

# Display chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# User input
if prompt := st.chat_input("ì—°ë§ì •ì‚°ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”!"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            
            # Convert streamlit history to LangChain history format for current run
            current_history = get_session_history(session_id)
            # (Optional: Sync session_state messages to history object if needed, 
            # but usually RunnableWithMessageHistory handles persistence. 
            # Here we just use the manual history passing for simplicity or update it.)
            
            # Since we are managing history manually in session_state for UI, 
            # we need to ensure the chain gets the correct history format.
            # However, create_retrieval_chain expects 'chat_history' in input 
            # if we don't use RunnableWithMessageHistory wrapper.
            # Let's manually construct chat_history from session_state for the invoke.
            
            from langchain_core.messages import HumanMessage, AIMessage
            chat_history = []
            for msg in st.session_state.messages[:-1]: # Exclude current prompt
                if msg["role"] == "user":
                    chat_history.append(HumanMessage(content=msg["content"]))
                elif msg["role"] == "assistant":
                    chat_history.append(AIMessage(content=msg["content"]))

            response_dict = chain.invoke({"input": prompt, "chat_history": chat_history})
            response = response_dict["answer"]
            
            # Source attribution
            sources = response_dict.get('context', [])
            if sources:
                with st.expander("ì°¸ê³  ìë£Œ"):
                    seen_sources = set()
                    for i, doc in enumerate(sources):
                        source_name = doc.metadata.get('source', 'Unknown')
                        if source_name not in seen_sources:
                            st.write(f"**ì¶œì²˜:** {os.path.basename(source_name)}")
                            st.caption(doc.page_content[:200] + "...")
                            seen_sources.add(source_name)

            st.markdown(response)
            st.session_state.messages.append({"role": "assistant", "content": response})

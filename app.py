import streamlit as st
import os
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.chains import create_history_aware_retriever
from langchain_core.documents import Document

load_dotenv()

st.set_page_config(page_title="2025 ì—°ë§ì •ì‚° ìƒë‹´ ì±—ë´‡", page_icon="ğŸ’°")

st.title("ğŸ’° 2025ë…„ ì—°ë§ì •ì‚° ìƒë‹´ ì±—ë´‡")
st.markdown("""
2024ë…„ ê·€ì† ì—°ë§ì •ì‚° ì‹ ê³ ì•ˆë‚´ ë° 2025ë…„ ê°œì •ì„¸ë²•ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•´ë“œë¦½ë‹ˆë‹¤.\n
**ì£¼ì˜:** ì •í™•í•œ ì„¸ë¬´ ìƒë‹´ì€ ì „ë¬¸ê°€ì™€ ìƒì˜í•˜ì„¸ìš”.
""")

# Initialize Chat Chain
@st.cache_resource
def get_chain():
    # 1. ë²¡í„° ì €ì¥ì†Œ ì„¤ì •
    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma(persist_directory="chroma_db", embedding_function=embeddings)
    # kê°’ì„ 10ìœ¼ë¡œ ëŠ˜ë ¤ '2025ë…„ì— ë°”ë€ ì ' ê°™ì€ í¬ê´„ì ì¸ ì§ˆë¬¸ì— ëŒ€í•´ ì¶©ë¶„í•œ ë¬¸ë§¥ì„ í™•ë³´í•©ë‹ˆë‹¤.
    retriever = vectorstore.as_retriever(search_kwargs={"k": 10})
    
    # 2. LLM ì„¤ì •
    llm = ChatOpenAI(temperature=0, model_name="gpt-4o")

    # 3. ì§ˆë¬¸ ë§¥ë½í™” (ëŒ€í™” ê¸°ë¡ ë°˜ì˜)
    # ì´ ì²´ì¸ì€ ëŒ€í™” ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì¬êµ¬ì„±í•˜ì—¬ ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
    contextualize_q_system_prompt = """Given a chat history and the latest user question \
    which might reference context in the chat history, formulate a standalone question \
    which can be understood without the chat history. Do NOT answer the question, \
    just reformulate it if needed and otherwise return it as is."""
    
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )
    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )

    # 4. ì§ˆë¬¸ ë‹µë³€ ì²´ì¸ (Stuff Documents Chain)
    # ì´ ì²´ì¸ì€ ë¬¸ì„œì™€ ì§ˆë¬¸ì„ ë°›ì•„ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    qa_system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ ì—°ë§ì •ì‚° ë° ì„¸ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
    ì•„ë˜ ì œê³µëœ [ë¬¸ë§¥(Context)]ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

    [ë¬¸ë§¥(Context)]:
    {context}
    
    [ë‹µë³€ ê·œì¹™]:
    1. **ìµœìš°ì„  ìˆœìœ„**: '2025ë…„ ê°œì •ì„¸ë²•' ë˜ëŠ” '2025ë…„ ê·€ì†'ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì´ ë¬¸ë§¥ì— ìˆë‹¤ë©´, ì´ë¥¼ 2024ë…„ ìë£Œë³´ë‹¤ ìš°ì„ í•˜ì—¬ ìì„¸íˆ ì„¤ëª…í•˜ì„¸ìš”.
    2. ì§ˆë¬¸ì´ 'ê°œì •ëœ ë‚´ìš©'ì´ë‚˜ 'ë‹¬ë¼ì§„ ì 'ì„ ë¬»ëŠ”ë‹¤ë©´, ë¬¸ë§¥ì—ì„œ 'ê°œì •', 'ì‹ ì„¤', 'í™•ëŒ€', 'ì¸ìƒ' ë“±ì˜ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ì •ë¦¬í•´ ì£¼ì„¸ìš”.
    3. ë¬¸ë§¥ì— ì •ë‹µì´ ì—†ë‹¤ë©´ ì†”ì§í•˜ê²Œ "ì œê³µëœ ìë£Œì—ì„œ í•´ë‹¹ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•˜ì„¸ìš”. (ë‹¨, 2025ë…„ ê°œì • ë‚´ìš©ì´ ì¡°ê¸ˆì´ë¼ë„ ë³´ì´ë©´ ìµœëŒ€í•œ í™œìš©í•˜ì„¸ìš”.)
    4. ë‹µë³€ì€ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
    """
    
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

    # 5. ìµœì¢… ê²€ìƒ‰ ì²´ì¸
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    
    return rag_chain

# ëŒ€í™” ê¸°ë¡ì„ ìœ„í•œ ì„¸ì…˜ ìƒíƒœ
if "messages" not in st.session_state:
    st.session_state.messages = []

if "store" not in st.session_state:
    st.session_state.store = {}

session_id = "user_session"
if session_id not in st.session_state.store:
    st.session_state.store[session_id] = ChatMessageHistory()

# Streamlitì´ ë‹¤ì‹œ ì‹¤í–‰ë  ë•Œ ê¸°ë¡ì„ ìˆ˜ë™ìœ¼ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ í—¬í¼ í•¨ìˆ˜
def get_session_history(session_id: str) -> BaseChatMessageHistory:
    return st.session_state.store[session_id]

chain = get_chain()

# ëŒ€í™” ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ì—°ë§ì •ì‚°ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”!"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            
            # í˜„ì¬ ì‹¤í–‰ì„ ìœ„í•´ Streamlit ê¸°ë¡ì„ LangChain ê¸°ë¡ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            current_history = get_session_history(session_id)
            # (ì„ íƒ ì‚¬í•­: í•„ìš”í•œ ê²½ìš° ì„¸ì…˜ ìƒíƒœ ë©”ì‹œì§€ë¥¼ íˆìŠ¤í† ë¦¬ ê°ì²´ì— ë™ê¸°í™”í•˜ì§€ë§Œ, 
            # ë³´í†µ RunnableWithMessageHistoryê°€ ì§€ì†ì„±ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. 
            # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ ìˆ˜ë™ìœ¼ë¡œ ê¸°ë¡ì„ ì „ë‹¬í•˜ê±°ë‚˜ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.)
            
            # UIë¥¼ ìœ„í•´ session_stateì—ì„œ ê¸°ë¡ì„ ìˆ˜ë™ìœ¼ë¡œ ê´€ë¦¬í•˜ë¯€ë¡œ, 
            # ì²´ì¸ì´ ì˜¬ë°”ë¥¸ ê¸°ë¡ í˜•ì‹ì„ ë°›ë„ë¡ í•´ì•¼ í•©ë‹ˆë‹¤.
            # í•˜ì§€ë§Œ create_retrieval_chainì€ RunnableWithMessageHistory ë˜í¼ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš° 
            # ì…ë ¥ì— 'chat_history'ê°€ í•„ìš”í•©ë‹ˆë‹¤.
            # invokeë¥¼ ìœ„í•´ session_stateì—ì„œ chat_historyë¥¼ ìˆ˜ë™ìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
            
            from langchain_core.messages import HumanMessage, AIMessage
            chat_history = []
            for msg in st.session_state.messages[:-1]: # í˜„ì¬ í”„ë¡¬í”„íŠ¸ ì œì™¸
                if msg["role"] == "user":
                    chat_history.append(HumanMessage(content=msg["content"]))
                elif msg["role"] == "assistant":
                    chat_history.append(AIMessage(content=msg["content"]))

            # RAG Chain Invoke
            # "2025"ì™€ "ê°œì •"ì´ í¬í•¨ëœ ì§ˆë¬¸ì¸ ê²½ìš°, 2025ë…„ ì „ì²´ ë°ì´í„°ë¥¼ ë¬¸ë§¥ì— ì¶”ê°€í•˜ì—¬ ìš”ì•½ ë‹µë³€ ìœ ë„
            if "2025" in prompt and ("ê°œì •" in prompt or "ë‹¬ë¼ì§„" in prompt or "ë³€í™”" in prompt):
                with open("data/2025ë…„ ê°œì •ì„¸ë²•.txt", "r", encoding="utf-8") as f:
                    full_2025_text = f.read()
                
                # ë³„ë„ì˜ RAG ì²´ì¸ì„ íƒ€ì§€ ì•Šê³ , ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ LLMì— ì§ì ‘ ì „ë‹¬í•˜ì—¬ ìš”ì•½í•©ë‹ˆë‹¤.
                # (ê¸°ì¡´ RAG ì²´ì¸ì„ ì¬ì‚¬ìš©í•˜ë ¤ë‹¤ ë‚´ë¶€ ë³€ìˆ˜ ë¶ˆì¼ì¹˜ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìœ¼ë¯€ë¡œ ë‹¨ìˆœí™”)
                
                # LLM ì§ì ‘ í˜¸ì¶œ
                messages = [
                    ("system", """ë‹¹ì‹ ì€ í•œêµ­ ì—°ë§ì •ì‚° ë° ì„¸ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì•„ë˜ ì œê³µëœ [2025ë…„ ê°œì •ì„¸ë²• ì „ë¬¸]ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„¸íˆ ìš”ì•½í•´ì„œ ë‹µë³€í•˜ì„¸ìš”.
ë‚´ìš©ì´ ë§ìœ¼ë¯€ë¡œ í•µì‹¬ì ì¸ ë³€í™” ìœ„ì£¼ë¡œ, ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì˜ ì •ë¦¬í•´ì„œ ë‹µë³€í•˜ì„¸ìš”.

[2025ë…„ ê°œì •ì„¸ë²• ì „ë¬¸]:
""" + full_2025_text),
                    ("human", prompt)
                ]
                
                # ë³„ë„ì˜ LLM ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
                temp_llm = ChatOpenAI(temperature=0, model_name="gpt-4o")
                response_msg = temp_llm.invoke(messages)
                response = response_msg.content
                
                # ì¶œì²˜ í‘œê¸°ìš© ê°€ì§œ context
                sources = [Document(page_content="2025ë…„ ê°œì •ì„¸ë²• ì „ì²´ ë°ì´í„° (ìš”ì•½ ëª¨ë“œ)", metadata={"source": "data/2025ë…„ ê°œì •ì„¸ë²•.txt"})]
                response_dict = {"answer": response, "context": sources}

            else:
                # ì¼ë°˜ì ì¸ RAG ì‹¤í–‰
                response_dict = chain.invoke({"input": prompt, "chat_history": chat_history})
                response = response_dict["answer"]
                sources = response_dict.get('context', [])
            
            # ì¶œì²˜ í‘œê¸° (Source attribution)
            if sources:
                with st.expander("ì°¸ê³  ìë£Œ"):
                    seen_sources = set()
                    for i, doc in enumerate(sources):
                        source_name = doc.metadata.get('source', 'Unknown')
                        if source_name not in seen_sources:
                            st.write(f"**ì¶œì²˜:** {os.path.basename(source_name)}")
                            st.caption(doc.page_content[:200] + "...")
                            seen_sources.add(source_name)

            st.markdown(response)
            st.session_state.messages.append({"role": "assistant", "content": response})
